{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"If you have time, please check my other notebooks.\n\n* Train : https://www.kaggle.com/kojimar/fb3-single-pytorch-model-train\n* Inference : https://www.kaggle.com/kojimar/fb3-single-pytorch-model-inference","metadata":{}},{"cell_type":"markdown","source":"References\n* https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train\n* https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-inference","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\n%env TOKENIZERS_PARALLELISM=false\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{"papermill":{"duration":0.003425,"end_time":"2022-09-08T02:59:40.459399","exception":false,"start_time":"2022-09-08T02:59:40.455974","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Deberta Family ver. 13\n\nSep. 28, LB 21th\n\n* CFG1 : 10 fold deberta-v3-base CV/LB: 0.4595/0.44\n* CFG2 : 10 fold deberta-v3-large CV/LB: 0.4553/0.44\n* CFG3 : 10 fold deberta-v2-xlarge CV/LB: 0.4604/0.44\n* CFG4 : 10 fold deberta-v3-base FGM CV/LB: 0.4590/0.44\n* CFG5 : 10 fold deberta-v3-large FGM CV/LB: 0.4564/0.44\n* CFG6 : 10 fold deberta-v2-xlarge CV/LB: 0.4666/0.44\n* CFG7 : 10 fold deberta-v2-xlarge-mnli CV/LB: 0.4675/0.44\n* CFG8 : 10 fold deberta-v3-large unscale CV/LB: 0.4616/0.43\n* CFG9 : 10 fold deberta-v3-large unscale CV/LB: 0.4548/0.43\n* CFG10 : 10 fold deberta-v3-large unscale CV/LB: 0.4569/0.43","metadata":{}},{"cell_type":"code","source":"class CFG1:\n    model = \"microsoft/deberta-v3-base\"\n    path = \"../input/0911-deberta-v3-base/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-base/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=24\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG2:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0911-deberta-v3-large/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG3:\n    model = \"microsoft/deberta-v2-xlarge\"\n    path = \"../input/0911-deberta-v2-xlarge/\"\n    base = \"../input/fb3models/microsoft-deberta-v2-xlarge/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=4\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n\nclass CFG4:\n    model = \"microsoft/deberta-v3-base\"\n    path = \"../input/0913-deberta-v3-base-fgm/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-base/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=24\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG5:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0914-deberta-v3-large-fgm/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG6:\n    model = \"microsoft/deberta-v2-xlarge\"\n    path = \"../input/0919-deberta-v2-xlarge/\"\n    base = \"../input/fb3models/microsoft-deberta-v2-xlarge/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=4\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG7:\n    model = \"microsoft/deberta-v2-xlarge-mnli\"\n    path = \"../input/0919-deberta-v2-xlarge-mnli/\"\n    base = \"../input/fb3models/microsoft-deberta-v2-xlarge/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=4\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG8:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0925-deberta-v3-large-unscale/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG9:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0926-deberta-v3-large-unscale/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG10:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0927-deberta-v3-large-unscale/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nCFG_list = [CFG1, CFG2, CFG3, CFG4, CFG5, CFG6, CFG7, CFG8, CFG9, CFG10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"papermill":{"duration":0.003431,"end_time":"2022-09-08T02:59:41.602809","exception":false,"start_time":"2022-09-08T02:59:41.599378","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.execute_input":"2022-09-08T02:59:41.611471Z","iopub.status.busy":"2022-09-08T02:59:41.611164Z","iopub.status.idle":"2022-09-08T02:59:41.623929Z","shell.execute_reply":"2022-09-08T02:59:41.623039Z"},"papermill":{"duration":0.019843,"end_time":"2022-09-08T02:59:41.626234","exception":false,"start_time":"2022-09-08T02:59:41.606391","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OOF","metadata":{"papermill":{"duration":0.003606,"end_time":"2022-09-08T02:59:41.633413","exception":false,"start_time":"2022-09-08T02:59:41.629807","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# oof\n# ====================================================\nfor CFG in CFG_list:\n    oof_df = pd.read_pickle(CFG.path+'oof_df.pkl')\n    labels = oof_df[CFG.target_cols].values\n    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n    score, scores = get_score(labels, preds)\n    LOGGER.info(f'Model: {CFG.model} Score: {score:<.4f}  Scores: {scores}')","metadata":{"execution":{"iopub.execute_input":"2022-09-08T02:59:41.641788Z","iopub.status.busy":"2022-09-08T02:59:41.641503Z","iopub.status.idle":"2022-09-08T02:59:41.881075Z","shell.execute_reply":"2022-09-08T02:59:41.880102Z"},"papermill":{"duration":0.245898,"end_time":"2022-09-08T02:59:41.883068","exception":false,"start_time":"2022-09-08T02:59:41.637170","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.003671,"end_time":"2022-09-08T02:59:41.891081","exception":false,"start_time":"2022-09-08T02:59:41.887410","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        return_tensors=None, \n        add_special_tokens=True, \n        #max_length=CFG.max_len,\n        #pad_to_max_length=True,\n        #truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['full_text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs","metadata":{"execution":{"iopub.execute_input":"2022-09-08T02:59:41.899956Z","iopub.status.busy":"2022-09-08T02:59:41.899644Z","iopub.status.idle":"2022-09-08T02:59:41.907222Z","shell.execute_reply":"2022-09-08T02:59:41.906157Z"},"papermill":{"duration":0.014446,"end_time":"2022-09-08T02:59:41.909458","exception":false,"start_time":"2022-09-08T02:59:41.895012","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.003578,"end_time":"2022-09-08T02:59:41.916767","exception":false,"start_time":"2022-09-08T02:59:41.913189","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim = 1)\n        return max_embeddings\n    \nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e-4\n        min_embeddings, _ = torch.min(embeddings, dim = 1)\n        return min_embeddings\n        \n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n            LOGGER.info(self.config)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path, output_hidden_states=True)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 6)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output","metadata":{"execution":{"iopub.execute_input":"2022-09-08T02:59:41.925993Z","iopub.status.busy":"2022-09-08T02:59:41.925699Z","iopub.status.idle":"2022-09-08T02:59:41.943110Z","shell.execute_reply":"2022-09-08T02:59:41.942275Z"},"papermill":{"duration":0.024494,"end_time":"2022-09-08T02:59:41.945014","exception":false,"start_time":"2022-09-08T02:59:41.920520","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# inference","metadata":{"papermill":{"duration":0.003537,"end_time":"2022-09-08T02:59:41.952301","exception":false,"start_time":"2022-09-08T02:59:41.948764","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.execute_input":"2022-09-08T02:59:41.961258Z","iopub.status.busy":"2022-09-08T02:59:41.960823Z","iopub.status.idle":"2022-09-08T02:59:41.966892Z","shell.execute_reply":"2022-09-08T02:59:41.965938Z"},"papermill":{"duration":0.012906,"end_time":"2022-09-08T02:59:41.968960","exception":false,"start_time":"2022-09-08T02:59:41.956054","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _idx, CFG in enumerate(CFG_list):\n    test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n    submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n    # sort by length to speed up inference\n    test['tokenize_length'] = [len(CFG.tokenizer(text)['input_ids']) for text in test['full_text'].values]\n    test = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n\n    test_dataset = TestDataset(CFG, test)\n    test_loader = DataLoader(test_dataset,\n                             batch_size=CFG.batch_size,\n                             shuffle=False,\n                             collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n                             num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n    predictions = []\n    for fold in CFG.trn_fold:\n        model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n        state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                           map_location=torch.device('cpu'))\n        model.load_state_dict(state['model'])\n        prediction = inference_fn(test_loader, model, device)\n        predictions.append(prediction)\n        del model, state, prediction; gc.collect()\n        torch.cuda.empty_cache()\n    predictions = np.mean(predictions, axis=0)\n    test[CFG.target_cols] = predictions\n    submission = submission.drop(columns=CFG.target_cols).merge(test[['text_id'] + CFG.target_cols], on='text_id', how='left')\n    display(submission.head())\n    submission[['text_id'] + CFG.target_cols].to_csv(f'submission_{_idx + 1}.csv', index=False)\n    del test, submission, predictions, test_dataset, test_loader; gc.collect()\n    torch.cuda.empty_cache() ","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-08T02:59:41.977476Z","iopub.status.busy":"2022-09-08T02:59:41.977221Z","iopub.status.idle":"2022-09-08T03:07:32.012265Z","shell.execute_reply":"2022-09-08T03:07:32.011153Z"},"papermill":{"duration":470.042141,"end_time":"2022-09-08T03:07:32.014810","exception":false,"start_time":"2022-09-08T02:59:41.972669","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{"papermill":{"duration":0.007035,"end_time":"2022-09-08T03:07:32.029966","exception":false,"start_time":"2022-09-08T03:07:32.022931","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\nsubmission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n\nsub1 = pd.read_csv(f'submission_1.csv')[CFG1.target_cols] * CFG1.weight\nsub2 = pd.read_csv(f'submission_2.csv')[CFG2.target_cols] * CFG2.weight\nsub3 = pd.read_csv(f'submission_3.csv')[CFG3.target_cols] * CFG3.weight\nsub4 = pd.read_csv(f'submission_4.csv')[CFG4.target_cols] * CFG4.weight\nsub5 = pd.read_csv(f'submission_5.csv')[CFG5.target_cols] * CFG5.weight\nsub6 = pd.read_csv(f'submission_6.csv')[CFG6.target_cols] * CFG6.weight\nsub7 = pd.read_csv(f'submission_7.csv')[CFG7.target_cols] * CFG7.weight\nsub8 = pd.read_csv(f'submission_8.csv')[CFG8.target_cols] * CFG8.weight\nsub9 = pd.read_csv(f'submission_9.csv')[CFG9.target_cols] * CFG9.weight\nsub10 = pd.read_csv(f'submission_10.csv')[CFG10.target_cols] * CFG10.weight\n\nens = (sub1 + sub2 + sub3 + sub4 + sub5 + sub6 + sub7 + sub8 + sub9 + sub10)/(CFG1.weight + CFG2.weight + CFG3.weight + CFG4.weight + CFG5.weight + CFG6.weight + CFG7.weight + CFG8.weight + CFG9.weight + CFG10.weight)\n#ens = (sub1 + sub2)/(CFG1.weight + CFG2.weight)\n\nsubmission[CFG1.target_cols] = ens\ndisplay(submission.head())\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2022-09-08T03:07:32.046022Z","iopub.status.busy":"2022-09-08T03:07:32.045680Z","iopub.status.idle":"2022-09-08T03:07:32.076162Z","shell.execute_reply":"2022-09-08T03:07:32.075275Z"},"papermill":{"duration":0.041042,"end_time":"2022-09-08T03:07:32.078302","exception":false,"start_time":"2022-09-08T03:07:32.037260","status":"completed"},"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]}]}